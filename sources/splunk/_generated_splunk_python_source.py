# ==============================================================================
# Merged Lakeflow Source: splunk
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import Any, Iterator
import json

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from requests.auth import HTTPBasicAuth
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/splunk/splunk.py
    ########################################################

    class LakeflowConnect:
        """
        Splunk Metrics Catalog connector for Lakeflow.
        Retrieves metrics, dimensions, dimension values, and rollup policies from Splunk REST API.
        """

        def __init__(self, options: dict) -> None:
            """
            Initialize the Splunk connector with connection parameters.

            Args:
                options: Dictionary containing:
                    - host: Splunk server hostname or IP address
                    - port: Splunk management port (default: 8089)
                    - username: Splunk username with appropriate permissions
                    - password: Splunk password
                    - verify_ssl: Whether to verify SSL certificates (default: False)
            """
            self.host = options.get("host")
            self.port = options.get("port", 8089)
            self.username = options.get("username")
            self.password = options.get("password")
            self.verify_ssl = options.get("verify_ssl", "false").lower() == "true" if isinstance(
                options.get("verify_ssl"), str
            ) else options.get("verify_ssl", False)

            if not self.host:
                raise ValueError("Missing required option: 'host'")
            if not self.username:
                raise ValueError("Missing required option: 'username'")
            if not self.password:
                raise ValueError("Missing required option: 'password'")

            self.base_url = f"https://{self.host}:{self.port}"
            self.auth = HTTPBasicAuth(self.username, self.password)

            # Default page size for pagination
            self.page_size = 100

        def list_tables(self) -> list[str]:
            """
            Returns a list of available tables from the Splunk Metrics Catalog.

            Returns:
                List of table names: metrics, dimensions, rollup_policies
            """
            return ["metrics", "dimensions", "rollup_policies"]

        def get_table_schema(self, table_name: str, table_options: dict[str, str] = {}) -> StructType:
            """
            Fetch the schema of a table.

            Args:
                table_name: The name of the table to fetch the schema for.

            Returns:
                A StructType object representing the schema of the table.
            """
            schemas = {
                "metrics": StructType(
                    [
                        StructField("metric_name", StringType(), False),
                        StructField("index", StringType(), True),
                        StructField(
                            "dimensions",
                            ArrayType(
                                StructType(
                                    [
                                        StructField("name", StringType(), True),
                                        StructField("value", StringType(), True),
                                    ]
                                )
                            ),
                            True,
                        ),
                    ]
                ),
                "dimensions": StructType(
                    [
                        StructField("dimension_name", StringType(), False),
                    ]
                ),
                "rollup_policies": StructType(
                    [
                        StructField("index", StringType(), False),
                        StructField("defaultAggregation", StringType(), True),
                        StructField("metricList", StringType(), True),
                        StructField("metricListType", StringType(), True),
                        StructField("dimensionList", StringType(), True),
                        StructField("dimensionListType", StringType(), True),
                        StructField(
                            "summaries",
                            ArrayType(
                                StructType(
                                    [
                                        StructField("rollupIndex", StringType(), True),
                                        StructField("span", StringType(), True),
                                    ]
                                )
                            ),
                            True,
                        ),
                    ]
                ),
            }

            if table_name not in schemas:
                raise ValueError(f"Table '{table_name}' is not supported.")

            return schemas[table_name]

        def read_table_metadata(self, table_name: str, table_options: dict[str, str] = {}) -> dict:
            """
            Fetch the metadata of a table.

            Args:
                table_name: The name of the table to fetch the metadata for.

            Returns:
                A dictionary containing primary_key and ingestion_type.
                All Splunk Metrics Catalog tables use snapshot ingestion.
            """
            metadata = {
                "metrics": {
                    "primary_keys": ["metric_name"],
                    "ingestion_type": "snapshot",
                },
                "dimensions": {
                    "primary_keys": ["dimension_name"],
                    "ingestion_type": "snapshot",
                },
                "rollup_policies": {
                    "primary_keys": ["index"],
                    "ingestion_type": "snapshot",
                },
            }

            if table_name not in metadata:
                raise ValueError(f"Table '{table_name}' is not supported.")

            return metadata[table_name]

        def read_table(self, table_name: str, start_offset: dict, table_options: dict[str, str] = {}) -> tuple[Iterator[dict], dict]:
            """
            Read the records of a table and return an iterator of records and an offset.

            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from.

            Returns:
                An iterator of records in JSON format and an offset dictionary.
            """
            if table_name == "metrics":
                return self._read_metrics(start_offset)
            elif table_name == "dimensions":
                return self._read_dimensions(start_offset)
            elif table_name == "rollup_policies":
                return self._read_rollup_policies(start_offset)
            else:
                raise ValueError(f"Table '{table_name}' is not supported.")

        def _make_request(self, endpoint: str, params: dict = None) -> dict:
            """
            Make a GET request to the Splunk REST API.

            Args:
                endpoint: API endpoint path
                params: Query parameters

            Returns:
                JSON response as dictionary
            """
            url = f"{self.base_url}{endpoint}"

            if params is None:
                params = {}

            # Always request JSON output
            params["output_mode"] = "json"

            response = requests.get(
                url,
                auth=self.auth,
                params=params,
                verify=self.verify_ssl,
            )

            if response.status_code == 401:
                raise Exception("Splunk API authentication failed. Check username and password.")
            elif response.status_code == 403:
                raise Exception(
                    "Splunk API access forbidden. Ensure user has required capabilities "
                    "(list_metrics_catalog for read operations)."
                )
            elif response.status_code != 200:
                raise Exception(f"Splunk API error: {response.status_code} {response.text}")

            return response.json()

        def _read_metrics(self, start_offset: dict) -> tuple[Iterator[dict], dict]:
            """
            Read metrics from the Splunk Metrics Catalog.

            Args:
                start_offset: Offset containing 'offset' key for pagination

            Returns:
                Iterator of metric records and next offset
            """
            offset = 0
            if start_offset and "offset" in start_offset:
                offset = start_offset["offset"]

            all_records = []
            current_offset = offset

            while True:
                params = {
                    "count": self.page_size,
                    "offset": current_offset,
                }

                data = self._make_request("/services/catalog/metricstore/metrics", params)
                entries = data.get("entry", [])

                if not entries:
                    break

                for entry in entries:
                    content = entry.get("content", {})
                    record = self._parse_metric_entry(entry, content)
                    all_records.append(record)

                if len(entries) < self.page_size:
                    break

                current_offset += self.page_size

            # For snapshot tables, return None offset to indicate completion
            # or the same offset to signal no more data
            return iter(all_records), {"offset": current_offset, "completed": True}

        def _parse_metric_entry(self, entry: dict, content: dict) -> dict:
            """
            Parse a metric entry from the API response.

            Args:
                entry: The entry object from the response
                content: The content object within the entry

            Returns:
                Parsed metric record
            """
            metric_name = entry.get("name") or content.get("metric_name", "")
            index = content.get("index", None)

            # Parse dimensions if present
            dimensions = None
            dims_data = content.get("dimensions")
            if dims_data and isinstance(dims_data, dict):
                dimensions = [
                    {"name": k, "value": str(v) if v is not None else None}
                    for k, v in dims_data.items()
                ]

            return {
                "metric_name": metric_name,
                "index": index,
                "dimensions": dimensions,
            }

        def _read_dimensions(self, start_offset: dict) -> tuple[Iterator[dict], dict]:
            """
            Read dimensions from the Splunk Metrics Catalog.

            Args:
                start_offset: Offset containing 'offset' key for pagination

            Returns:
                Iterator of dimension records and next offset
            """
            offset = 0
            if start_offset and "offset" in start_offset:
                offset = start_offset["offset"]

            all_records = []
            current_offset = offset

            while True:
                params = {
                    "count": self.page_size,
                    "offset": current_offset,
                }

                data = self._make_request("/services/catalog/metricstore/dimensions", params)
                entries = data.get("entry", [])

                if not entries:
                    break

                for entry in entries:
                    content = entry.get("content", {})
                    dimension_name = entry.get("name") or content.get("dimension_name", "")

                    record = {
                        "dimension_name": dimension_name,
                    }
                    all_records.append(record)

                if len(entries) < self.page_size:
                    break

                current_offset += self.page_size

            return iter(all_records), {"offset": current_offset, "completed": True}

        def _read_rollup_policies(self, start_offset: dict) -> tuple[Iterator[dict], dict]:
            """
            Read rollup policies from the Splunk Metrics Catalog.

            Args:
                start_offset: Offset containing 'offset' key for pagination

            Returns:
                Iterator of rollup policy records and next offset
            """
            offset = 0
            if start_offset and "offset" in start_offset:
                offset = start_offset["offset"]

            all_records = []
            current_offset = offset

            while True:
                params = {
                    "count": self.page_size,
                    "offset": current_offset,
                }

                data = self._make_request("/services/catalog/metricstore/rollup", params)
                entries = data.get("entry", [])

                if not entries:
                    break

                for entry in entries:
                    content = entry.get("content", {})
                    record = self._parse_rollup_entry(entry, content)
                    all_records.append(record)

                if len(entries) < self.page_size:
                    break

                current_offset += self.page_size

            return iter(all_records), {"offset": current_offset, "completed": True}

        def _parse_rollup_entry(self, entry: dict, content: dict) -> dict:
            """
            Parse a rollup policy entry from the API response.

            Args:
                entry: The entry object from the response
                content: The content object within the entry

            Returns:
                Parsed rollup policy record
            """
            index_name = entry.get("name") or content.get("index", "")

            # Parse summaries if present
            summaries = None
            summaries_data = content.get("summaries")
            if summaries_data and isinstance(summaries_data, dict):
                summaries = []
                for key, value in summaries_data.items():
                    if isinstance(value, dict):
                        summary = {
                            "rollupIndex": value.get("rollupIndex", None),
                            "span": value.get("span", None),
                        }
                        summaries.append(summary)

            return {
                "index": index_name,
                "defaultAggregation": content.get("defaultAggregation", None),
                "metricList": content.get("metricList", None),
                "metricListType": content.get("metricListType", None),
                "dimensionList": content.get("dimensionList", None),
                "dimensionListType": content.get("dimensionListType", None),
                "summaries": summaries,
            }


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
