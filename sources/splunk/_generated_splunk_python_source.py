# ==============================================================================
# Merged Lakeflow Source: splunk
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import Any, Iterator
import json

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/splunk/splunk.py
    ########################################################

    class LakeflowConnect:
        """
        SignalFx connector for Lakeflow.
        Retrieves organization members from SignalFx API.
        """

        def __init__(self, options: dict) -> None:
            """
            Initialize the SignalFx connector with connection parameters.

            Args:
                options: Dictionary containing:
                    - api_token: SignalFx API token for authentication
                    - base_url: SignalFx API base URL (default: https://api.us1.signalfx.com)
            """
            self.api_token = options.get("api_token")
            self.base_url = options.get("base_url", "https://api.us1.signalfx.com")

            if not self.api_token:
                raise ValueError("Missing required option: 'api_token'")

            # Set up headers for authentication
            self.headers = {
                "X-SF-TOKEN": self.api_token,
                "Content-Type": "application/json"
            }

            # Default page size for pagination
            self.page_size = 100

        def list_tables(self) -> list[str]:
            """
            Returns a list of available tables from the SignalFx API.

            Returns:
                List of table names: members, teams, dashboards, metrics
            """
            return ["members", "teams", "dashboards", "metrics"]

        def get_table_schema(self, table_name: str, table_options: dict[str, str] = {}) -> StructType:
            """
            Fetch the schema of a table.

            Args:
                table_name: The name of the table to fetch the schema for.
                table_options: Additional options for the table.

            Returns:
                A StructType object representing the schema of the table.
            """
            schemas = {
                "members": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("organizationId", StringType(), True),
                        StructField("fullName", StringType(), True),
                        StructField("email", StringType(), True),
                        StructField("created", LongType(), True),
                        StructField("lastUpdated", LongType(), True),
                        StructField("admin", BooleanType(), True),
                        StructField("readOnly", BooleanType(), True),
                        StructField("creator", StringType(), True),
                        StructField("title", StringType(), True),
                        StructField("roles", StringType(), True),
                        StructField("title_description", StringType(), True),
                        StructField("role_description", StringType(), True),
                    ]
                ),
                "teams": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("members", StringType(), True),
                        StructField("description", StringType(), True),
                        StructField("created", LongType(), True),
                        StructField("lastUpdated", LongType(), True),
                        StructField("creator", StringType(), True),
                    ]
                ),
                "dashboards": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("description", StringType(), True),
                        StructField("created", LongType(), True),
                        StructField("lastUpdated", LongType(), True),
                        StructField("creator", StringType(), True),
                        StructField("groupId", StringType(), True),
                        StructField("tags", StringType(), True),
                        StructField("eventOverlays", StringType(), True),
                    ]
                ),
                "metrics": StructType(
                    [
                        StructField("name", StringType(), False),
                        StructField("type", StringType(), True),
                        StructField("description", StringType(), True),
                        StructField("created", LongType(), True),
                        StructField("lastUpdated", LongType(), True),
                        StructField("creator", StringType(), True),
                    ]
                ),
            }

            if table_name not in schemas:
                raise ValueError(f"Table '{table_name}' is not supported.")

            return schemas[table_name]

        def read_table_metadata(self, table_name: str, table_options: dict[str, str] = {}) -> dict:
            """
            Fetch the metadata of a table.

            Args:
                table_name: The name of the table to fetch the metadata for.
                table_options: Additional options for the table.

            Returns:
                A dictionary containing primary_keys and ingestion_type.
                SignalFx members use snapshot ingestion.
            """
            metadata = {
                "members": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "teams": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "dashboards": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "metrics": {
                    "primary_keys": ["name"],
                    "ingestion_type": "snapshot",
                },
            }

            if table_name not in metadata:
                raise ValueError(f"Table '{table_name}' is not supported.")

            return metadata[table_name]

        def read_table(self, table_name: str, start_offset: dict, table_options: dict[str, str] = {}) -> tuple[Iterator[dict], dict]:
            """
            Read the records of a table and return an iterator of records and an offset.

            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from.
                table_options: Additional options for the table.

            Returns:
                An iterator of records in JSON format and an offset dictionary.
            """
            if table_name == "members":
                return self._read_members(start_offset)
            elif table_name == "teams":
                return self._read_teams(start_offset)
            elif table_name == "dashboards":
                return self._read_dashboards(start_offset)
            elif table_name == "metrics":
                return self._read_metrics(start_offset)
            else:
                raise ValueError(f"Table '{table_name}' is not supported.")

        def _make_request(self, endpoint: str, params: dict = None) -> dict:
            """
            Make a GET request to the SignalFx API.

            Args:
                endpoint: API endpoint path
                params: Query parameters

            Returns:
                JSON response as dictionary
            """
            url = f"{self.base_url}{endpoint}"

            if params is None:
                params = {}

            response = requests.get(
                url,
                headers=self.headers,
                params=params,
            )

            if response.status_code == 401:
                raise Exception("SignalFx API authentication failed. Check API token.")
            elif response.status_code == 403:
                raise Exception(
                    "SignalFx API access forbidden. Ensure API token has appropriate permissions."
                )
            elif response.status_code != 200:
                raise Exception(f"SignalFx API error: {response.status_code} {response.text}")

            return response.json()

        def _read_members(self, start_offset: dict) -> tuple[Iterator[dict], dict]:
            """
            Read organization members from SignalFx API.

            Args:
                start_offset: Offset containing pagination info (not used for full refresh)

            Returns:
                Iterator of member records and next offset
            """
            all_records = []

            # Call the SignalFx members API endpoint
            data = self._make_request("/v2/organization/member")

            # The API returns a list of member objects
            members = data if isinstance(data, list) else data.get("results", [])

            for member in members:
                record = {
                    "id": member.get("id"),
                    "organizationId": member.get("organizationId"),
                    "fullName": member.get("fullName"),
                    "email": member.get("email"),
                    "created": member.get("created"),
                    "lastUpdated": member.get("lastUpdated"),
                    "admin": member.get("admin", False),
                    "readOnly": member.get("readOnly", False),
                    "creator": member.get("creator"),
                    "title": member.get("title"),
                    "roles": member.get("roles"),
                    "title_description": member.get("roles.title"),
                    "role_description": member.get("roles.description"),
                }
                all_records.append(record)

            # For snapshot tables, return completed offset
            return iter(all_records), {"completed": True}

        def _read_teams(self, start_offset: dict) -> tuple[Iterator[dict], dict]:
            """
            Read teams from SignalFx API.

            Args:
                start_offset: Offset containing pagination info (not used for full refresh)

            Returns:
                Iterator of team records and next offset
            """
            all_records = []

            # Call the SignalFx teams API endpoint
            data = self._make_request("/v2/team")

            # The API returns a list of team objects
            teams = data if isinstance(data, list) else data.get("results", [])

            for team in teams:
                record = {
                    "id": team.get("id"),
                    "name": team.get("name"),
                    "members": team.get("members"),
                    "description": team.get("description"),
                    "created": team.get("created"),
                    "lastUpdated": team.get("lastUpdated"),
                    "creator": team.get("creator"),
                }
                all_records.append(record)

            # For snapshot tables, return completed offset
            return iter(all_records), {"completed": True}

        def _read_dashboards(self, start_offset: dict) -> tuple[Iterator[dict], dict]:
            """
            Read dashboards from SignalFx API.

            Args:
                start_offset: Offset containing pagination info (not used for full refresh)

            Returns:
                Iterator of dashboard records and next offset
            """
            all_records = []

            # Call the SignalFx dashboards API endpoint
            data = self._make_request("/v2/dashboard")

            # The API returns a list of dashboard objects
            dashboards = data if isinstance(data, list) else data.get("results", [])

            for dashboard in dashboards:
                # Convert tags list to comma-separated string if present
                tags = dashboard.get("tags", [])
                tags_str = ",".join(tags) if isinstance(tags, list) else tags

                record = {
                    "id": dashboard.get("id"),
                    "name": dashboard.get("name"),
                    "description": dashboard.get("description"),
                    "created": dashboard.get("created"),
                    "lastUpdated": dashboard.get("lastUpdated"),
                    "creator": dashboard.get("creator"),
                    "groupId": dashboard.get("groupId"),
                    "tags": dashboard.get("tags"),
                    "eventOverlays": dashboard.get("eventOverlays"),
                }
                all_records.append(record)

            # For snapshot tables, return completed offset
            return iter(all_records), {"completed": True}

        def _read_metrics(self, start_offset: dict) -> tuple[Iterator[dict], dict]:
            """
            Read metrics from SignalFx API.

            Args:
                start_offset: Offset containing pagination info (not used for full refresh)

            Returns:
                Iterator of metric records and next offset
            """
            all_records = []

            # Call the SignalFx metrics API endpoint
            data = self._make_request("/v2/metric")

            # The API returns a list of metric objects
            metrics = data if isinstance(data, list) else data.get("results", [])

            for metric in metrics:
                record = {
                    "name": metric.get("name"),
                    "type": metric.get("type"),
                    "description": metric.get("description"),
                    "created": metric.get("created"),
                    "lastUpdated": metric.get("lastUpdated"),
                    "creator": metric.get("creator"),
                }
                all_records.append(record)

            # For snapshot tables, return completed offset
            return iter(all_records), {"completed": True}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
